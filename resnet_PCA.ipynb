{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO0kkcWrEair6A4jJKq62q9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eshal26/PCA-CNN/blob/main/resnet_PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "import os\n",
        "import shutil\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "d4j2BfAaUl1F"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = 'train_dataset'\n",
        "val_dir = 'validation_dataset'\n",
        "test_dir = 'test_dataset'\n",
        "\n",
        "# Define transformations for training, validation, and testing data\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = [0.8245, 0.8547, 0.9387], std = [0.1323, 0.1431, 0.0530])\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = [0.8245, 0.8547, 0.9387], std = [0.1323, 0.1431, 0.0530])\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(root=val_dir, transform=val_test_transform)\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=val_test_transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "GlkM_G1jUotS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVDiOZ5ZQ9Ml"
      },
      "outputs": [],
      "source": [
        "class SimpleResNetWithPCA(nn.Module):\n",
        "    def __init__(self, num_classes=2, pca_components=32, pca=None):\n",
        "        super(SimpleResNetWithPCA, self).__init__()\n",
        "\n",
        "        # First Residual Block\n",
        "        self.layer1 = BasicBlock(3, 64)\n",
        "\n",
        "        # Second Residual Block\n",
        "        self.layer2 = BasicBlock(64, 128)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.dropout1 = nn.Dropout(p=0.5)\n",
        "        self.dropout2 = nn.Dropout(p=0.5)\n",
        "\n",
        "        # PCA Parameters\n",
        "        self.pca = pca  # Accept PCA instance externally or set to None\n",
        "        self.pca_components = pca_components\n",
        "\n",
        "        # Pooling and Fully Connected Layers\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc1 = nn.Linear(pca_components, 32)  # Adjusted input features after PCA\n",
        "        self.fc2 = nn.Linear(32, num_classes)\n",
        "\n",
        "    def extract_features(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)  # Flatten before PCA\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.extract_features(x)\n",
        "        device = x.device\n",
        "\n",
        "        # Apply PCA if it is fitted\n",
        "        if self.pca is not None:\n",
        "            x_cpu = x.detach().cpu().numpy()  # Convert tensor to NumPy array\n",
        "            x_pca = self.pca.transform(x_cpu)  # Apply PCA transformation\n",
        "            x = torch.from_numpy(x_pca).to(device, dtype=torch.float32)  # Back to PyTorch tensor\n",
        "        else:\n",
        "            raise RuntimeError(\"PCA must be fitted before the forward pass\")\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def fit_pca(self, dataloader, num_components=32):\n",
        "        \"\"\"\n",
        "        Fit PCA on the extracted features of the model.\n",
        "        \"\"\"\n",
        "        self.eval()  # Ensure model is in eval mode\n",
        "        features = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, _ in dataloader:\n",
        "                inputs = inputs.to(next(self.parameters()).device)\n",
        "                x = self.extract_features(inputs)\n",
        "                features.append(x.cpu().numpy())\n",
        "\n",
        "        features = np.vstack(features)  # Combine all features\n",
        "        pca = PCA(n_components=num_components)\n",
        "        pca.fit(features)\n",
        "        self.pca = pca  # Assign the fitted PCA to the model\n",
        "\n",
        "\n",
        "model = SimpleResNetWithPCA(num_classes=2, pca_components=32)\n",
        "\n",
        "# Fit PCA on the features extracted from the training dataset\n",
        "model.fit_pca(train_loader, num_components=32)\n",
        "\n",
        "# Now the model is ready to apply PCA during the forward pass\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Training Loop\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track statistics\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "            total_samples += inputs.size(0)\n",
        "\n",
        "        train_loss = running_loss / total_samples\n",
        "        train_acc = running_corrects.double() / total_samples\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_corrects = 0\n",
        "        val_samples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Track statistics\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                val_corrects += torch.sum(preds == labels.data)\n",
        "                val_samples += inputs.size(0)\n",
        "\n",
        "        val_loss = val_loss / val_samples\n",
        "        val_acc = val_corrects.double() / val_samples\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Step 1: Define the model without a fitted PCA\n",
        "model = SimpleResNetWithPCA(num_classes=2, pca_components=32)\n",
        "\n",
        "# Step 2: Fit PCA on training data\n",
        "model.fit_pca(train_loader, num_components=32)\n",
        "\n",
        "# Step 3: Use the model for training\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Step 4: Train the model\n",
        "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device=device)\n"
      ],
      "metadata": {
        "id": "ItnyN3EiVA8E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}