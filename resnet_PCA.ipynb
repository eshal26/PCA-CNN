{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eshal26/PCA-CNN/blob/main/resnet_PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "import os\n",
        "import shutil\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "d4j2BfAaUl1F"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = 'train_dataset'\n",
        "val_dir = 'validation_dataset'\n",
        "test_dir = 'test_dataset'\n",
        "\n",
        "# Define transformations for training, validation, and testing data\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = [0.8245, 0.8547, 0.9387], std = [0.1323, 0.1431, 0.0530])\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = [0.8245, 0.8547, 0.9387], std = [0.1323, 0.1431, 0.0530])\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(root=val_dir, transform=val_test_transform)\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=val_test_transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "GlkM_G1jUotS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JVDiOZ5ZQ9Ml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ec2a3ee-ebae-4317-b37b-834a1163a033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleResNetWithPCA(\n",
            "  (layer1): BasicBlock(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (shortcut): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): BasicBlock(\n",
            "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (shortcut): Sequential(\n",
            "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (dropout1): Dropout(p=0.5, inplace=False)\n",
            "  (dropout2): Dropout(p=0.5, inplace=False)\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc1): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# Define BasicBlock\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Adjust shortcut to match dimensions if needed\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        shortcut = self.shortcut(x)\n",
        "        out += shortcut\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# Define SimpleResNetWithPCA\n",
        "class SimpleResNetWithPCA(nn.Module):\n",
        "    def __init__(self, num_classes=2, pca_components=32, pca=None):\n",
        "        super(SimpleResNetWithPCA, self).__init__()\n",
        "\n",
        "        # First Residual Block\n",
        "        self.layer1 = BasicBlock(3, 64)\n",
        "\n",
        "        # Second Residual Block\n",
        "        self.layer2 = BasicBlock(64, 128)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.dropout1 = nn.Dropout(p=0.5)\n",
        "        self.dropout2 = nn.Dropout(p=0.5)\n",
        "\n",
        "        # PCA Parameters\n",
        "        self.pca = pca  # Accept PCA instance externally or set to None\n",
        "        self.pca_components = pca_components\n",
        "\n",
        "        # Pooling and Fully Connected Layers\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc1 = nn.Linear(pca_components, 32)  # Adjusted input features after PCA\n",
        "        self.fc2 = nn.Linear(32, num_classes)\n",
        "\n",
        "    def extract_features(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)  # Flatten before PCA\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.extract_features(x)\n",
        "        device = x.device\n",
        "\n",
        "        # Apply PCA if it is fitted\n",
        "        if self.pca is not None:\n",
        "            x_cpu = x.detach().cpu().numpy()  # Convert tensor to NumPy array\n",
        "            x_pca = self.pca.transform(x_cpu)  # Apply PCA transformation\n",
        "            x = torch.from_numpy(x_pca).to(device, dtype=torch.float32)  # Back to PyTorch tensor\n",
        "        else:\n",
        "            raise RuntimeError(\"PCA must be fitted before the forward pass\")\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def fit_pca(self, dataloader, num_components=32):\n",
        "        \"\"\"\n",
        "        Fit PCA on the extracted features of the model.\n",
        "        \"\"\"\n",
        "        self.eval()  # Ensure model is in eval mode\n",
        "        features = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, _ in dataloader:\n",
        "                inputs = inputs.to(next(self.parameters()).device)\n",
        "                x = self.extract_features(inputs)\n",
        "                features.append(x.cpu().numpy())\n",
        "\n",
        "        features = np.vstack(features)  # Combine all features\n",
        "        pca = PCA(n_components=num_components)\n",
        "        pca.fit(features)\n",
        "        self.pca = pca  # Assign the fitted PCA to the model\n",
        "\n",
        "\n",
        "# Example usage\n",
        "# Assuming `train_loader` is defined\n",
        "model = SimpleResNetWithPCA(num_classes=2, pca_components=32)\n",
        "\n",
        "# Fit PCA on the features extracted from the training dataset\n",
        "model.fit_pca(train_loader, num_components=32)\n",
        "\n",
        "# Now the model is ready to apply PCA during the forward pass\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Training Loop\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track statistics\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "            total_samples += inputs.size(0)\n",
        "\n",
        "        train_loss = running_loss / total_samples\n",
        "        train_acc = running_corrects.double() / total_samples\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_corrects = 0\n",
        "        val_samples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Track statistics\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                val_corrects += torch.sum(preds == labels.data)\n",
        "                val_samples += inputs.size(0)\n",
        "\n",
        "        val_loss = val_loss / val_samples\n",
        "        val_acc = val_corrects.double() / val_samples\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "num_epochs = 30\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Step 1: Define the model without a fitted PCA\n",
        "model = SimpleResNetWithPCA(num_classes=2, pca_components=32)\n",
        "\n",
        "# Step 2: Fit PCA on training data\n",
        "model.fit_pca(train_loader, num_components=32)\n",
        "\n",
        "# Step 3: Use the model for training\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Step 4: Train the model\n",
        "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device=device)\n"
      ],
      "metadata": {
        "id": "ItnyN3EiVA8E",
        "outputId": "4ebb2ab8-37c5-42ad-dca2-d6e40b4ec391",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "------------------------------\n",
            "Train Loss: 0.5434 Acc: 0.7646\n",
            "Val Loss: 0.4203 Acc: 0.8462\n",
            "Epoch 2/30\n",
            "------------------------------\n",
            "Train Loss: 0.4132 Acc: 0.8475\n",
            "Val Loss: 0.3499 Acc: 0.8523\n",
            "Epoch 3/30\n",
            "------------------------------\n",
            "Train Loss: 0.3750 Acc: 0.8510\n",
            "Val Loss: 0.3186 Acc: 0.8708\n",
            "Epoch 4/30\n",
            "------------------------------\n",
            "Train Loss: 0.3379 Acc: 0.8606\n",
            "Val Loss: 0.2950 Acc: 0.8585\n",
            "Epoch 5/30\n",
            "------------------------------\n",
            "Train Loss: 0.3189 Acc: 0.8756\n",
            "Val Loss: 0.2699 Acc: 0.8800\n",
            "Epoch 6/30\n",
            "------------------------------\n",
            "Train Loss: 0.2963 Acc: 0.8760\n",
            "Val Loss: 0.2498 Acc: 0.8862\n",
            "Epoch 7/30\n",
            "------------------------------\n",
            "Train Loss: 0.2895 Acc: 0.8833\n",
            "Val Loss: 0.2326 Acc: 0.8985\n",
            "Epoch 8/30\n",
            "------------------------------\n",
            "Train Loss: 0.2643 Acc: 0.8975\n",
            "Val Loss: 0.2157 Acc: 0.9077\n",
            "Epoch 9/30\n",
            "------------------------------\n",
            "Train Loss: 0.2561 Acc: 0.8978\n",
            "Val Loss: 0.2027 Acc: 0.9354\n",
            "Epoch 10/30\n",
            "------------------------------\n",
            "Train Loss: 0.2425 Acc: 0.9086\n",
            "Val Loss: 0.1900 Acc: 0.9446\n",
            "Epoch 11/30\n",
            "------------------------------\n",
            "Train Loss: 0.2301 Acc: 0.9171\n",
            "Val Loss: 0.1742 Acc: 0.9354\n",
            "Epoch 12/30\n",
            "------------------------------\n",
            "Train Loss: 0.2186 Acc: 0.9136\n",
            "Val Loss: 0.1646 Acc: 0.9477\n",
            "Epoch 13/30\n",
            "------------------------------\n",
            "Train Loss: 0.2126 Acc: 0.9194\n",
            "Val Loss: 0.1578 Acc: 0.9446\n",
            "Epoch 14/30\n",
            "------------------------------\n",
            "Train Loss: 0.2093 Acc: 0.9282\n",
            "Val Loss: 0.1476 Acc: 0.9600\n",
            "Epoch 15/30\n",
            "------------------------------\n",
            "Train Loss: 0.1968 Acc: 0.9251\n",
            "Val Loss: 0.1396 Acc: 0.9538\n",
            "Epoch 16/30\n",
            "------------------------------\n",
            "Train Loss: 0.1903 Acc: 0.9336\n",
            "Val Loss: 0.1316 Acc: 0.9538\n",
            "Epoch 17/30\n",
            "------------------------------\n",
            "Train Loss: 0.1964 Acc: 0.9255\n",
            "Val Loss: 0.1289 Acc: 0.9508\n",
            "Epoch 18/30\n",
            "------------------------------\n",
            "Train Loss: 0.1841 Acc: 0.9347\n",
            "Val Loss: 0.1252 Acc: 0.9600\n",
            "Epoch 19/30\n",
            "------------------------------\n",
            "Train Loss: 0.1773 Acc: 0.9324\n",
            "Val Loss: 0.1180 Acc: 0.9569\n",
            "Epoch 20/30\n",
            "------------------------------\n",
            "Train Loss: 0.1755 Acc: 0.9336\n",
            "Val Loss: 0.1180 Acc: 0.9631\n",
            "Epoch 21/30\n",
            "------------------------------\n",
            "Train Loss: 0.1755 Acc: 0.9316\n",
            "Val Loss: 0.1124 Acc: 0.9662\n",
            "Epoch 22/30\n",
            "------------------------------\n",
            "Train Loss: 0.1699 Acc: 0.9351\n",
            "Val Loss: 0.1071 Acc: 0.9631\n",
            "Epoch 23/30\n",
            "------------------------------\n",
            "Train Loss: 0.1649 Acc: 0.9401\n",
            "Val Loss: 0.1070 Acc: 0.9600\n",
            "Epoch 24/30\n",
            "------------------------------\n",
            "Train Loss: 0.1669 Acc: 0.9374\n",
            "Val Loss: 0.1017 Acc: 0.9662\n",
            "Epoch 25/30\n",
            "------------------------------\n",
            "Train Loss: 0.1565 Acc: 0.9370\n",
            "Val Loss: 0.0989 Acc: 0.9662\n",
            "Epoch 26/30\n",
            "------------------------------\n",
            "Train Loss: 0.1482 Acc: 0.9439\n",
            "Val Loss: 0.0938 Acc: 0.9631\n",
            "Epoch 27/30\n",
            "------------------------------\n",
            "Train Loss: 0.1559 Acc: 0.9401\n",
            "Val Loss: 0.0970 Acc: 0.9631\n",
            "Epoch 28/30\n",
            "------------------------------\n",
            "Train Loss: 0.1561 Acc: 0.9443\n",
            "Val Loss: 0.0960 Acc: 0.9662\n",
            "Epoch 29/30\n",
            "------------------------------\n",
            "Train Loss: 0.1608 Acc: 0.9374\n",
            "Val Loss: 0.0921 Acc: 0.9662\n",
            "Epoch 30/30\n",
            "------------------------------\n",
            "Train Loss: 0.1462 Acc: 0.9428\n",
            "Val Loss: 0.0933 Acc: 0.9631\n"
          ]
        }
      ]
    }
  ]
}