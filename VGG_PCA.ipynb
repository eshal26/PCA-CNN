{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eshal26/PCA-CNN/blob/main/VGG_PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "import os\n",
        "import shutil\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "L-9ZU3dWjLUX"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = 'train_dataset'\n",
        "val_dir = 'validation_dataset'\n",
        "test_dir = 'test_dataset'\n",
        "\n",
        "# Define transformations for training, validation, and testing data\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.8238, 0.8539, 0.9391], std=[0.1325, 0.1437, 0.0529])\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.8238, 0.8539, 0.9391], std=[0.1325, 0.1437, 0.0529])\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(root=val_dir, transform=val_test_transform)\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=val_test_transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "MLiSkobUjS-J"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA, IncrementalPCA\n",
        "\n",
        "class CustomVGGWithPCA(nn.Module):\n",
        "    def __init__(self, num_classes=2, pca_components=40):\n",
        "        super(CustomVGGWithPCA, self).__init__()\n",
        "\n",
        "        # Feature extraction layers with added pooling to reduce dimensions\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 112x112x64\n",
        "\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 56x56x128\n",
        "\n",
        "            # Additional pooling layer to reduce dimensionality further\n",
        "            nn.MaxPool2d(kernel_size=4, stride=4)   # Output: 14x14x128\n",
        "        )\n",
        "\n",
        "        # Initialize PCA to None\n",
        "        self.pca = None\n",
        "\n",
        "        # Classifier layers\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(pca_components, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def extract_features(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the features for PCA\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.extract_features(x)\n",
        "        device = x.device\n",
        "\n",
        "        # Apply PCA if it is fitted\n",
        "        if self.pca is not None:\n",
        "\n",
        "\n",
        "            # Convert tensor to NumPy array (detach and move to CPU)\n",
        "            x_cpu = x.detach().cpu().numpy()\n",
        "\n",
        "            # Apply PCA transformation\n",
        "            x_pca = self.pca.transform(x_cpu)  # Apply PCA transformation on NumPy array\n",
        "\n",
        "            # Convert the transformed data back to PyTorch tensor\n",
        "            x = torch.from_numpy(x_pca).to(device, dtype=torch.float32)\n",
        "        else:\n",
        "            raise RuntimeError(\"PCA must be fitted before the forward pass\")\n",
        "\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "AsdUctVzjUpl"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "def fit_pca_on_train_data(train_loader, model, device, pca_components ,  batch_size=64):\n",
        "    # Dynamically determine the number of features from a sample batch\n",
        "    with torch.no_grad():\n",
        "        sample_batch, _ = next(iter(train_loader))\n",
        "        sample_features = model.extract_features(sample_batch.to(device))\n",
        "        num_features = sample_features.shape[1]  # Get feature dimension from sample batch\n",
        "\n",
        "\n",
        "    # Initialize IncrementalPCA with the number of features dynamically set\n",
        "    ipca = IncrementalPCA(n_components=pca_components, batch_size=batch_size)\n",
        "\n",
        "    # Process each batch and fit IncrementalPCA\n",
        "    for images, _ in train_loader:\n",
        "        images = images.to(device)\n",
        "        with torch.no_grad():\n",
        "            features = model.extract_features(images)\n",
        "            features = features.detach().cpu().numpy()  # Convert to numpy array for IncrementalPCA\n",
        "            ipca.partial_fit(features)  # Fit incrementally\n",
        "\n",
        "    # Assign the fitted IncrementalPCA back to modelâ€™s PCA attribute\n",
        "    model.pca = ipca\n",
        "    print(\"PCA fitting completed on training data.\")\n",
        "\n",
        "# Example usage\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pca_components = 40\n",
        "num_classes = 2\n",
        "\n",
        "model = CustomVGGWithPCA(num_classes=num_classes, pca_components=pca_components).to(device)\n",
        "fit_pca_on_train_data(train_loader,  model,  device, pca_components=pca_components)\n"
      ],
      "metadata": {
        "id": "q6Gvr93ljXu9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9a734f3-3862-4c3d-f148-ac64f26f47b7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PCA fitting completed on training data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss, correct_predictions, total_samples = 0.0, 0, 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_accuracy = 100 * correct_predictions / total_samples\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}: Training Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQrD5vO7jbAh",
        "outputId": "ad6fb5eb-2825-4f9f-a820-8ed8ca489ba8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40: Training Loss: 1.3678, Accuracy: 77.04%\n",
            "Epoch 2/40: Training Loss: 0.9917, Accuracy: 77.50%\n",
            "Epoch 3/40: Training Loss: 0.7822, Accuracy: 79.07%\n",
            "Epoch 4/40: Training Loss: 0.6591, Accuracy: 80.34%\n",
            "Epoch 5/40: Training Loss: 0.6228, Accuracy: 80.11%\n",
            "Epoch 6/40: Training Loss: 0.5220, Accuracy: 81.87%\n",
            "Epoch 7/40: Training Loss: 0.5104, Accuracy: 81.72%\n",
            "Epoch 8/40: Training Loss: 0.4799, Accuracy: 82.26%\n",
            "Epoch 9/40: Training Loss: 0.4716, Accuracy: 82.18%\n",
            "Epoch 10/40: Training Loss: 0.4834, Accuracy: 82.03%\n",
            "Epoch 11/40: Training Loss: 0.4651, Accuracy: 82.60%\n",
            "Epoch 12/40: Training Loss: 0.4679, Accuracy: 82.49%\n",
            "Epoch 13/40: Training Loss: 0.4736, Accuracy: 82.87%\n",
            "Epoch 14/40: Training Loss: 0.4721, Accuracy: 83.14%\n",
            "Epoch 15/40: Training Loss: 0.4380, Accuracy: 83.99%\n",
            "Epoch 16/40: Training Loss: 0.4572, Accuracy: 82.68%\n",
            "Epoch 17/40: Training Loss: 0.4614, Accuracy: 82.37%\n",
            "Epoch 18/40: Training Loss: 0.4583, Accuracy: 83.29%\n",
            "Epoch 19/40: Training Loss: 0.4555, Accuracy: 82.41%\n",
            "Epoch 20/40: Training Loss: 0.4400, Accuracy: 83.41%\n",
            "Epoch 21/40: Training Loss: 0.4356, Accuracy: 83.33%\n",
            "Epoch 22/40: Training Loss: 0.4616, Accuracy: 82.91%\n",
            "Epoch 23/40: Training Loss: 0.4301, Accuracy: 82.87%\n",
            "Epoch 24/40: Training Loss: 0.4059, Accuracy: 84.56%\n",
            "Epoch 25/40: Training Loss: 0.4288, Accuracy: 82.22%\n",
            "Epoch 26/40: Training Loss: 0.4317, Accuracy: 83.64%\n",
            "Epoch 27/40: Training Loss: 0.4349, Accuracy: 82.37%\n",
            "Epoch 28/40: Training Loss: 0.4112, Accuracy: 83.41%\n",
            "Epoch 29/40: Training Loss: 0.4288, Accuracy: 82.80%\n",
            "Epoch 30/40: Training Loss: 0.4227, Accuracy: 84.10%\n",
            "Epoch 31/40: Training Loss: 0.4443, Accuracy: 83.03%\n",
            "Epoch 32/40: Training Loss: 0.4037, Accuracy: 83.79%\n",
            "Epoch 33/40: Training Loss: 0.4150, Accuracy: 85.02%\n",
            "Epoch 34/40: Training Loss: 0.4280, Accuracy: 83.26%\n",
            "Epoch 35/40: Training Loss: 0.3979, Accuracy: 84.87%\n",
            "Epoch 36/40: Training Loss: 0.4103, Accuracy: 83.83%\n",
            "Epoch 37/40: Training Loss: 0.4213, Accuracy: 84.02%\n",
            "Epoch 38/40: Training Loss: 0.3950, Accuracy: 84.14%\n",
            "Epoch 39/40: Training Loss: 0.4181, Accuracy: 84.49%\n",
            "Epoch 40/40: Training Loss: 0.4128, Accuracy: 84.49%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zwkXtNxOjdWd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}